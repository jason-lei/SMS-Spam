{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Classifier\n",
    "Project for E4990 Introduction to Data Science Industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team Members:\n",
    "Jason Lei and Andres Soto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roles:\n",
    "Machine Learning, Natural Language Processing, Web Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset:\n",
    "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Description and Motivation\n",
    "While reading about available open-source datasets, we discovered the SMS Spam Dataset, which contains text messages labeled as ham (legitimate) and spam. SMS spam is an increasingly relevant problem as the use of mobile devices continues to rise. SMS spam, however, is not just a nuisance--it can have immediate financial consequences for its recipients, as many mobile phone users are charged based on how many texts they receive. Furthermore, SMS spam is also commonly involved with phishing schemes that lure users into clicking on potentially malicious links. Thus, the detection of SMS spam is important to help ensure user privacy and financial security for an always-rising number of people. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audience\n",
    "Anyone with a mobile device that receives SMS messages. Telecommunications companies trying to reduce the amount of spam their customers receive. Cyber security companies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "Naive Bayes, K Nearest Neighbor, SVM, Adaboost, Tree-based classifier \n",
    "\n",
    "At the bottom of the notebook is a sample model based on word frequency. We will also be trying an NLP-based approach using LIWC (Linguistic Inquiry Word Count) analysis. This will allow us to see if the psychological meaning of words, as well as intent, can help us better predict spam messages better than sheer word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface\n",
    "Our initial interface is to provide a web application where users can copy-paste text in and receive a prediction of whether the text message is spam or ham, along with a confidence estimate. Given that neither of us is very experienced with web development, we imagine that getting this simple interface could be potentially challening. Additional features we'd like to implement include outputting a word cloud indicating which parts of the text most strongly weigh the message towards being spam or ham. See below for an example word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "word-cloud.jpg",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('word-cloud.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "LIWC.jpg",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('LIWC.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Expansions of Project\n",
    "Given time, we would also like to expand our spam classification domain from SMS messages to email as well. We seek to create our own spam dataset consisting of our own scraped emails and the emails of a fake email account that we will sign up to receive a lot of spam. The final goal of this project would be to build an application that would allow users to authenticate into their email accounts (probably Gmail) and then provide a spam analysis of their inbox, as well as a breakdown of the topics that constitute their spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jason\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3130    LookAtMe!: Thanks for your purchase of a video...\n",
       "106           Aight, I'll hit you up when I get some cash\n",
       "4697                           Don no da:)whats you plan?\n",
       "856                         Going to take your babe out ?\n",
       "3454    No need lar. Jus testing e phone card. Dunno n...\n",
       "Name: v2, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "data_train, data_test, labels_train, labels_test = cross_validation.train_test_split(\n",
    "    df.v2,\n",
    "    df.v1, \n",
    "    test_size=0.1, \n",
    "    random_state=42)\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
    "# vectorizer.fit(data_train)\n",
    "data_train_transformed = vectorizer.fit_transform(data_train)\n",
    "data_test_transformed  = vectorizer.transform(data_test)\n",
    "\n",
    "# percentile of 100 chosen to avoid having to pickle selector. Can come back to this later\n",
    "selector = SelectPercentile(f_classif, percentile=20)\n",
    "# labels_train = pd.get_dummies(labels_train)\n",
    "selector.fit(data_train_transformed, labels_train)\n",
    "data_train_transformed = selector.transform(data_train_transformed).toarray()\n",
    "data_test_transformed  = selector.transform(data_test_transformed).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(data_train_transformed, labels_train)\n",
    "predictions = clf.predict(data_test_transformed)\n",
    "\n",
    "joblib.dump(vectorizer.vocabulary_, 'vectorizer.pkl')\n",
    "joblib.dump(selector, 'selector.pkl')\n",
    "joblib.dump(clf, 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Trying to reopen the model and vectorizer and run it on a random string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], \n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'v2': \"Aight, I'll hit you up when I get some cash\"}\n",
    "text_df = pd.DataFrame(data=d, index={1})\n",
    "\n",
    "vocabulary_to_load =joblib.load('vectorizer.pkl')\n",
    "vectorizer2 = TfidfVectorizer(vocabulary=vocabulary_to_load)\n",
    "# vectorizer2._validate_vocabulary()\n",
    "vectorizer2.fit(vocabulary_to_load)\n",
    "text_transformed = vectorizer2.transform(text_df.v2)\n",
    "\n",
    "clf2 = joblib.load('model.pkl')\n",
    "selector2 = joblib.load('selector.pkl')\n",
    "text_transformed = selector2.transform(text_transformed).toarray()\n",
    "output = clf2.predict(text_transformed)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d = {'v2': \"this is a random string\"}\n",
    "# text_df = pd.DataFrame(data=d, index={0})\n",
    "# vocabulary_to_load =joblib.load('vectorizer.pkl')\n",
    "# text_transformed_test = vectorizer.transform(text_df)\n",
    "# text_transformed_test  = selector.transform(text_transformed_test).toarray()\n",
    "# output = clf.predict(text_transformed_test)\n",
    "\n",
    "# output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
